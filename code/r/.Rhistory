for(subsample in cv.subsamples){
for(colsample_bytree in cv.colsample_bytrees){
cat("eta == ", eta, " max_depth == ", max_depth, " min_child_weight == ", min_child_weight) ;
cat("\n")
cat("max_delta_step == ", max_delta_step, " subsample == ", subsample,
" colsample_bytree == ", colsample_bytree) ;
cat("\n")
ptm <- proc.time()
param <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"eta" = eta,
"max_depth" = max_depth ,
"min_child_weight" = min_child_weight ,
"max_delta_step" = max_delta_step,
"subsample" = subsample ,
"colsample_bytree" = colsample_bytree,
"num_class" = 9,
"nthread" = 8)
bst.cv = xgb.cv(param=param, data = x.train, label = y.train,
nfold = cv.nfold, nrounds=cv.nround) ;
# cv.mloglossmean = as.numeric(bst.cv$test.mlogloss.mean) ;
#cv.mloglossstd = as.numeric(bst.cv$test.mlogloss.std) ;
cv.mlogloss = as.numeric(bst.cv$test.mlogloss.mean) + as.numeric(bst.cv$test.mlogloss.std) ;
#  cv.eval = data.frame(iround = c(1:cv.nround), mloglossmean = cv.mloglossmean,
#                      mloglossstd = cv.mloglossstd,
#                      mlogloss = cv.mlogloss)
cv.eval = data.frame(iround = c(1:cv.nround), mlogloss = cv.mlogloss) ;
cv.bestround = order(cv.eval$mlogloss) ;
cv.bestround = cv.bestround[1] ;
cv.bestmlogloss = cv.eval$mlogloss[cv.bestround] ;
cv.record = rbind(cv.record,
data.frame(eta = eta, max_depth = max_depth, min_child_weight = min_child_weight,
max_delta_step =max_delta_step, subsample = subsample,
colsample_bytree = colsample_bytree,
bestround = cv.bestround, bestloss = cv.bestmlogloss)) ;
cat("running time is ",proc.time() - ptm)
}
}
}
}
}
}
cv.record
q()
rm(list = ls())
library(xgboost)
library(methods)
library(mlbench)
train = read.csv("train_freq_v2.csv", header = TRUE,stringsAsFactors = F)
test = read.csv("test_freq_v2.csv", header = TRUE,stringsAsFactors = F)
y.train = train$target
y.train = gsub('Class_','',y.train)
y.train = as.integer(y.train)-1 #xgboost take features in [0,numOfClass-1]
x.train = subset(train, select = -c(id,setid,target))
x.train = as.matrix(x.train)
x.train = matrix(as.numeric(x.train),nrow(x.train),ncol(x.train))
dim(x.train)
head(x.train)
x.test = subset(test, select = -c(id))
x.test = as.matrix(x.test)
x.test = matrix(as.numeric(x.test),nrow(x.test),ncol(x.test))
dim(x.test)
head(x.test)
param <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"eta" = 0.01,
"max_depth" = 12 ,
"min_child_weight" = 1.3 ,
"max_delta_step" = 0,
"subsample" = 0.6,
"colsample_bytree" = 0.5,
"num_class" = 9,
"nthread" = 8)
#nround = 3044
nround = 2313
bst = xgboost(param=param, data = x.train, label = y.train, nrounds=nround)
dim(x.test)
head(x.test)
pred = predict(bst,x.test)
pred = matrix(pred,9,length(pred)/9)
pred = t(pred)
pred.result <- data.frame(id = test$id, pred)
names(pred.result)[2:10] = unique(train$target)
head(pred.result)
dim(pred.result)
write.csv(pred.result, file = "prediction_v22.csv", row.names = FALSE)
rm(list = ls())
library(xgboost)
library(methods)
library(mlbench)
train = read.csv("train_AllFreq.csv", header = TRUE,stringsAsFactors = F)
cv.etas = c(0.01) ;
cv.max_depths = c(12) ;
cv.min_child_weights = c(1.3) ;
cv.max_delta_steps = c(0) ;
cv.subsamples = c(0.6) ;
cv.colsample_bytrees = c(0.5) ;
cv.record = NULL ;
cv.nround = 3200 ;
cv.nfold = 5;
for(eta in cv.etas){
for(max_depth in cv.max_depths){
for(min_child_weight in cv.min_child_weights){
for(max_delta_step in cv.max_delta_steps){
for(subsample in cv.subsamples){
for(colsample_bytree in cv.colsample_bytrees){
cat("eta == ", eta, " max_depth == ", max_depth, " min_child_weight == ", min_child_weight) ;
cat("\n")
cat("max_delta_step == ", max_delta_step, " subsample == ", subsample,
" colsample_bytree == ", colsample_bytree) ;
cat("\n")
ptm <- proc.time()
param <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"eta" = eta,
"max_depth" = max_depth ,
"min_child_weight" = min_child_weight ,
"max_delta_step" = max_delta_step,
"subsample" = subsample ,
"colsample_bytree" = colsample_bytree,
"num_class" = 9,
"nthread" = 8)
bst.cv = xgb.cv(param=param, data = x.train, label = y.train,
nfold = cv.nfold, nrounds=cv.nround) ;
# cv.mloglossmean = as.numeric(bst.cv$test.mlogloss.mean) ;
#cv.mloglossstd = as.numeric(bst.cv$test.mlogloss.std) ;
cv.mlogloss = as.numeric(bst.cv$test.mlogloss.mean) + as.numeric(bst.cv$test.mlogloss.std) ;
#  cv.eval = data.frame(iround = c(1:cv.nround), mloglossmean = cv.mloglossmean,
#                      mloglossstd = cv.mloglossstd,
#                      mlogloss = cv.mlogloss)
cv.eval = data.frame(iround = c(1:cv.nround), mlogloss = cv.mlogloss) ;
cv.bestround = order(cv.eval$mlogloss) ;
cv.bestround = cv.bestround[1] ;
cv.bestmlogloss = cv.eval$mlogloss[cv.bestround] ;
cv.record = rbind(cv.record,
data.frame(eta = eta, max_depth = max_depth, min_child_weight = min_child_weight,
max_delta_step =max_delta_step, subsample = subsample,
colsample_bytree = colsample_bytree,
bestround = cv.bestround, bestloss = cv.bestmlogloss)) ;
cat("running time is ",proc.time() - ptm)
}
}
}
}
}
}
y.train = train$target
y.train = gsub('Class_','',y.train)
y.train = as.integer(y.train)-1 #xgboost take features in [0,numOfClass-1]
x.train = subset(train, select = -c(id,setid,target))
x.train = as.matrix(x.train)
x.train = matrix(as.numeric(x.train),nrow(x.train),ncol(x.train))
cv.etas = c(0.01) ;
cv.max_depths = c(12) ;
cv.min_child_weights = c(1.3) ;
cv.max_delta_steps = c(0) ;
cv.subsamples = c(0.6) ;
cv.colsample_bytrees = c(0.5) ;
cv.record = NULL ;
cv.nround = 3200 ;
cv.nfold = 5;
for(eta in cv.etas){
for(max_depth in cv.max_depths){
for(min_child_weight in cv.min_child_weights){
for(max_delta_step in cv.max_delta_steps){
for(subsample in cv.subsamples){
for(colsample_bytree in cv.colsample_bytrees){
cat("eta == ", eta, " max_depth == ", max_depth, " min_child_weight == ", min_child_weight) ;
cat("\n")
cat("max_delta_step == ", max_delta_step, " subsample == ", subsample,
" colsample_bytree == ", colsample_bytree) ;
cat("\n")
ptm <- proc.time()
param <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"eta" = eta,
"max_depth" = max_depth ,
"min_child_weight" = min_child_weight ,
"max_delta_step" = max_delta_step,
"subsample" = subsample ,
"colsample_bytree" = colsample_bytree,
"num_class" = 9,
"nthread" = 8)
bst.cv = xgb.cv(param=param, data = x.train, label = y.train,
nfold = cv.nfold, nrounds=cv.nround) ;
# cv.mloglossmean = as.numeric(bst.cv$test.mlogloss.mean) ;
#cv.mloglossstd = as.numeric(bst.cv$test.mlogloss.std) ;
cv.mlogloss = as.numeric(bst.cv$test.mlogloss.mean) + as.numeric(bst.cv$test.mlogloss.std) ;
#  cv.eval = data.frame(iround = c(1:cv.nround), mloglossmean = cv.mloglossmean,
#                      mloglossstd = cv.mloglossstd,
#                      mlogloss = cv.mlogloss)
cv.eval = data.frame(iround = c(1:cv.nround), mlogloss = cv.mlogloss) ;
cv.bestround = order(cv.eval$mlogloss) ;
cv.bestround = cv.bestround[1] ;
cv.bestmlogloss = cv.eval$mlogloss[cv.bestround] ;
cv.record = rbind(cv.record,
data.frame(eta = eta, max_depth = max_depth, min_child_weight = min_child_weight,
max_delta_step =max_delta_step, subsample = subsample,
colsample_bytree = colsample_bytree,
bestround = cv.bestround, bestloss = cv.bestmlogloss)) ;
cat("running time is ",proc.time() - ptm)
}
}
}
}
}
}
cv.recor
cv.record
q()
rm(list = ls())
library(xgboost)
library(methods)
library(mlbench)
train = read.csv("train_v1.csv", header = TRUE,stringsAsFactors = F) # without balanced
test <- read.csv("test.csv", header = TRUE)# the official test set
head(train)
head(test)
unique(train$feat_1)
unique(test$feat_1)
?intersect
dim(train)
dim(test)
for(i in 2:94){
cat(i, ' length union :', length(union(train[,i], train[,i])), ' length train ',
length(train[,i]), ' length test ', length(test[,i])) ;
}
for(i in 2:94){
cat(i, ' length union :', length(union(train[,i], train[,i])), ' length train ',
length(train[,i]), ' length test ', length(test[,i])) ;
cat('\n')
}
for(i in 2:94){
cat(i, ' length union :', length(union(train[,i], train[,i])), ' length train ',
length(unique(train[,i])), ' length test ', length(unique(test[,i]))) ;
cat('\n')
}
k = 0 ;
for(i in 2:94){
cat(i, ' length union :', length(union(train[,i], train[,i])), ' length train ',
length(unique(train[,i])), ' length test ', length(unique(test[,i]))) ;
cat('\n')
if(length(union(train[,i], train[,i])) > length(unique(train[,i]))){
k = k + 1 ;
}
if(length(union(train[,i], train[,i])) > length(unique(test[,i]))){
k = k + 1 ;
}
}
k
k = 0 ;
for(i in 2:94){
cat(i, ' length union :', length(union(train[,i], train[,i])), ' length train ',
length(unique(train[,i])), ' length test ', length(unique(test[,i]))) ;
cat('\n')
if(length(union(train[,i], train[,i])) > length(unique(train[,i])) ||
length(union(train[,i], train[,i])) > length(unique(test[,i])) ){
k = k + 1 ;
}
}
k
k = 0 ;
for(i in 2:94){
cat(i, ' length union :', length(union(train[,i], train[,i])), ' length train ',
length(unique(train[,i])), ' length test ', length(unique(test[,i]))) ;
cat('\n')
if(length(union(train[,i], train[,i])) < length(unique(train[,i])) &&
length(union(train[,i], train[,i])) < length(unique(test[,i])) ){
k = k + 1 ;
}
}
k
k = 0 ;
for(i in 2:94){
cat(i, ' length union :', length(union(train[,i], train[,i])), ' length train ',
length(unique(train[,i])), ' length test ', length(unique(test[,i]))) ;
cat('\n')
if(length(union(train[,i], train[,i])) <= length(unique(train[,i])) &&
length(union(train[,i], train[,i])) <= length(unique(test[,i])) ){
k = k + 1 ;
}
}
k
k = NULL ;
for(i in 2:94){
cat(i, ' length union :', length(union(train[,i], train[,i])), ' length train ',
length(unique(train[,i])), ' length test ', length(unique(test[,i]))) ;
cat('\n')
if(length(union(train[,i], train[,i])) > length(unique(train[,i])) &&
length(union(train[,i], train[,i])) > length(unique(test[,i])) ){
k = cbind(k,i);
}
}
k
k = c(0) ;
for(i in 2:94){
cat(i, ' length union :', length(union(train[,i], train[,i])), ' length train ',
length(unique(train[,i])), ' length test ', length(unique(test[,i]))) ;
cat('\n')
if(length(union(train[,i], train[,i])) > length(unique(train[,i])) &&
length(union(train[,i], train[,i])) > length(unique(test[,i])) ){
k = cbind(k,i);
}
}
k
unique(test[,i]
)
unique(train[,i])
order(unique(train[,1]))
order(unique(train[,2]))
order(unique(test[,2]))
sort(unique(test[,2]))
sort(unique(train[,2]))
dim(train)
rm(list = ls())
library(xgboost)
library(methods)
library(mlbench)
train = read.csv("train_v1.csv", header = TRUE,stringsAsFactors = F) # without balanced
names(train)
y.train = train$target
y.train = gsub('Class_','',y.train)
y.train = as.integer(y.train)-1 #xgboost take features in [0,numOfClass-1]
x.train = subset(train, select = -c(id,setid,target))
x.train = as.matrix(x.train)
x.train = matrix(as.numeric(x.train),nrow(x.train),ncol(x.train))
cv.etas = c(0.1) ;
cv.max_depths = c(12) ;
cv.min_child_weights = c(1.3) ;
cv.max_delta_steps = c(0) ;
cv.subsamples = c(0.6) ;
cv.colsample_bytrees = c(0.5) ;
cv.record = NULL ;
cv.nround = 2000 ;
cv.nfold = 5;
for(eta in cv.etas){
for(max_depth in cv.max_depths){
for(min_child_weight in cv.min_child_weights){
for(max_delta_step in cv.max_delta_steps){
for(subsample in cv.subsamples){
for(colsample_bytree in cv.colsample_bytrees){
cat("eta == ", eta, " max_depth == ", max_depth, " min_child_weight == ", min_child_weight) ;
cat("\n")
cat("max_delta_step == ", max_delta_step, " subsample == ", subsample,
" colsample_bytree == ", colsample_bytree) ;
cat("\n")
ptm <- proc.time()
param <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"eta" = eta,
"max_depth" = max_depth ,
"min_child_weight" = min_child_weight ,
"max_delta_step" = max_delta_step,
"subsample" = subsample ,
"colsample_bytree" = colsample_bytree,
"num_class" = 9,
"nthread" = 8)
bst.cv = xgb.cv(param=param, data = x.train, label = y.train,
nfold = cv.nfold, nrounds=cv.nround) ;
# cv.mloglossmean = as.numeric(bst.cv$test.mlogloss.mean) ;
#cv.mloglossstd = as.numeric(bst.cv$test.mlogloss.std) ;
cv.mlogloss = as.numeric(bst.cv$test.mlogloss.mean) + as.numeric(bst.cv$test.mlogloss.std) ;
#  cv.eval = data.frame(iround = c(1:cv.nround), mloglossmean = cv.mloglossmean,
#                      mloglossstd = cv.mloglossstd,
#                      mlogloss = cv.mlogloss)
cv.eval = data.frame(iround = c(1:cv.nround), mlogloss = cv.mlogloss) ;
cv.bestround = order(cv.eval$mlogloss) ;
cv.bestround = cv.bestround[1] ;
cv.bestmlogloss = cv.eval$mlogloss[cv.bestround] ;
cv.record = rbind(cv.record,
data.frame(eta = eta, max_depth = max_depth, min_child_weight = min_child_weight,
max_delta_step =max_delta_step, subsample = subsample,
colsample_bytree = colsample_bytree,
bestround = cv.bestround, bestloss = cv.bestmlogloss)) ;
cat("running time is ",proc.time() - ptm)
}
}
}
}
}
}
cv.etas = c(0.1) ;
cv.max_depths = c(9) ;
cv.min_child_weights = c(1.3) ;
cv.max_delta_steps = c(0) ;
cv.subsamples = c(0.6) ;
cv.colsample_bytrees = c(0.5) ;
cv.record = NULL ;
cv.nround = 2000 ;
cv.nfold = 5;
cv.etas = c(0.01) ;
cv.max_depths = c(12) ;
cv.min_child_weights = c(1.3) ;
cv.max_delta_steps = c(0) ;
cv.subsamples = c(0.6) ;
cv.colsample_bytrees = c(0.5) ;
cv.record = NULL ;
cv.nround = 2000 ;
cv.nfold = 5;
for(eta in cv.etas){
for(max_depth in cv.max_depths){
for(min_child_weight in cv.min_child_weights){
for(max_delta_step in cv.max_delta_steps){
for(subsample in cv.subsamples){
for(colsample_bytree in cv.colsample_bytrees){
cat("eta == ", eta, " max_depth == ", max_depth, " min_child_weight == ", min_child_weight) ;
cat("\n")
cat("max_delta_step == ", max_delta_step, " subsample == ", subsample,
" colsample_bytree == ", colsample_bytree) ;
cat("\n")
ptm <- proc.time()
param <- list("objective" = "multi:softprob",
"eval_metric" = "mlogloss",
"eta" = eta,
"max_depth" = max_depth ,
"min_child_weight" = min_child_weight ,
"max_delta_step" = max_delta_step,
"subsample" = subsample ,
"colsample_bytree" = colsample_bytree,
"num_class" = 9,
"nthread" = 8)
bst.cv = xgb.cv(param=param, data = x.train, label = y.train,
nfold = cv.nfold, nrounds=cv.nround) ;
# cv.mloglossmean = as.numeric(bst.cv$test.mlogloss.mean) ;
#cv.mloglossstd = as.numeric(bst.cv$test.mlogloss.std) ;
cv.mlogloss = as.numeric(bst.cv$test.mlogloss.mean) + as.numeric(bst.cv$test.mlogloss.std) ;
#  cv.eval = data.frame(iround = c(1:cv.nround), mloglossmean = cv.mloglossmean,
#                      mloglossstd = cv.mloglossstd,
#                      mlogloss = cv.mlogloss)
cv.eval = data.frame(iround = c(1:cv.nround), mlogloss = cv.mlogloss) ;
cv.bestround = order(cv.eval$mlogloss) ;
cv.bestround = cv.bestround[1] ;
cv.bestmlogloss = cv.eval$mlogloss[cv.bestround] ;
cv.record = rbind(cv.record,
data.frame(eta = eta, max_depth = max_depth, min_child_weight = min_child_weight,
max_delta_step =max_delta_step, subsample = subsample,
colsample_bytree = colsample_bytree,
bestround = cv.bestround, bestloss = cv.bestmlogloss)) ;
cat("running time is ",proc.time() - ptm)
}
}
}
}
}
}
cv.record
q()
rm(list = ls())
q()
q()
q()
setwd("/Users/mutian/Desktop/CodeStation/KaggleProp/code/r")
library(ggplot2)
library(randomForest)
library(readr)
install.packages('readr')
setwd("/Users/mutian/Desktop/CodeStation/KaggleProp/code/r")
library(ggplot2)
library(randomForest)
library(readr)
set.seed(1)
cat("Reading data\n")
train <- read_csv("../input/train.csv")
test <- read_csv("../input/test.csv")
train <- read_csv("../data/train/train.csv")
test <- read_csv("../data/test/test.csv")
train <- read_csv("../../data/train/train.csv")
test <- read_csv("../../data/test/test.csv")
head(train)
head(test)
shape(train)
dim(train)
dim(test)
class(train$T1_V1)
class(train$T1_V4)
train[,'T1_V4']
extractFeatures <- function(data) {
character_cols <- names(Filter(function(x) x=="character", sapply(data, class)))
for (col in character_cols) {
data[,col] <- as.factor(data[,col])
}
return(data)
}
trainFea <- extractFeatures(train)
testFea  <- extractFeatures(test)
trainFea.head
head(trainFea)
class(trainFea$T1_V4)
cat("Training model\n")
?randomForest
dim(train)
cat("Training model\n")
rf <- randomForest(trainFea[,3:34], trainFea$Hazard, ntree=100, imp=TRUE, sampsize=10000, do.trace=TRUE)
cat("Making predictions\n")
submission <- data.frame(Id=test$Id)
submission
submission$Hazard <- predict(rf, extractFeatures(testFea[,2:33]))
write_csv(submission, "1_random_forest_benchmark.csv")
write_csv(submission, "../../data/pred/1_random_forest_benchmark.csv")
cat("Plotting variable importance\n")
imp <- importance(rf, type=1)
featureImportance <- data.frame(Feature=row.names(imp), Importance=imp[,1])
p <- ggplot(featureImportance, aes(x=reorder(Feature, Importance), y=Importance)) +
geom_bar(stat="identity", fill="#53cfff") +
coord_flip() +
theme_light(base_size=20) +
xlab("Importance") +
ylab("") +
ggtitle("Random Forest Feature Importance\n") +
theme(plot.title=element_text(size=18))
ggsave("2_feature_importance.png", p, height=12, width=8, units="in")
ggsave("../../data/eda/2_feature_importance.png", p, height=12, width=8, units="in")
